<!DOCTYPE html>
<html>
    <head>
        <title>WebSD | Home</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="/web-stable-diffusion/assets/css/main.css">
        <link rel="stylesheet" href="/web-stable-diffusion/assets/css/group.css">
        <!-- <link rel="stylesheet" href="/web-stable-diffusion/css/table.css">          -->
        <link rel="shortcut icon" href="/web-stable-diffusion/assets/img/logo/mlc-favicon.png">
        <meta http-equiv="origin-trial" content="Agx76XA0ITxMPF0Z8rbbcMllwuxsyp9qdtQaXlLqu1JUrdHB6FPonuyIKJ3CsBREUkeioJck4nn3KO0c0kkwqAMAAABJeyJvcmlnaW4iOiJodHRwOi8vbG9jYWxob3N0Ojg4ODgiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5fQ==">
<meta http-equiv="origin-trial" content="AnmwqQ1dtYDQTYkZ5iMtHdINCaxjE94uWQBKp2yOz1wPTcjSRtOHUGQG+r2BxsEuM0qhxTVnuTjyh31HgTeA8gsAAABZeyJvcmlnaW4iOiJodHRwczovL21sYy5haTo0NDMiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZX0=">
    </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <div class="header d-flex
                        flex-column
                        flex-md-row justify-content-md-between">
              <a href="/" id="navtitle">
                  <img src="/web-stable-diffusion/assets/img/logo/mlc-logo-with-text-landscape.svg" height="70px"
                       alt="MLC" id="logo">
              </a>
              <ul id="topbar" class="nav nav-pills justify-content-center">
                    
                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link active"
                               href="/web-stable-diffusion/">
                                Home
                            </a>
                         
                        </li>

                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link "
                               href="https://github.com/mlc-ai/web-stable-diffusion">
                                Github
                            </a>
                         
                        </li>

                    

                </ul>
            </div>

            

            
            <!-- Schedule  -->
            
                <h1 id="web-stable-diffusion">Web Stable Diffusion</h1>

<p>This is the demo of bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support. Please checkout our <a href="https://github.com/mlc-ai/web-stable-diffusion">GitHub repo</a> to see how we did it. There is also a <a href="#text-to-image-generation-demo">demo</a> which you can try out.</p>

<p>We have been seeing amazing progress through AI models recently. Thanks to the open-source effort, developers can now easily compose open-source models together to produce amazing tasks. Stable diffusion enables the automatic creation of photorealistic images as well as images in various styles based on text input. These models are usually big and compute-heavy, which means we have to pipe through all computation requests to (GPU) servers when developing web applications based on these models. Additionally, most of the workloads have to run on a specific type of GPUs where popular deep-learning frameworks are readily available.</p>

<p>This project takes a step to change that status quo and bring more diversity to the ecosystem. There are a lot of reasons to bring some (or all) of the computation to the client side. There are many possible benefits, such as cost reduction on the service provider side, as well as an enhancement for personalization and privacy protection. The development of personal computers (even mobile devices) is going in the direction that enables such possibilities. The client side is getting pretty powerful. For example, the latest MacBook Pro can have up to 96GB of unified RAM that can be used to store the model weights and a reasonably powerful GPU to run many of the workloads.</p>

<p>Wouldn’t it be fun to directly bring the ML models to the client, have the user open a browser tab, and directly run the stable diffusion models on the browser? This project provides the first affirmative answer to this question.</p>

<h2 id="text-to-image-generation-demo">Text to Image Generation Demo</h2>

<p>Because WebGPU is not yet fully stable, nor have there ever been such large-scale AI models running on top of WebGPU, so we are testing the limit here. It may not work in your environment. So far, we have only tested it on Apple silicon. Please also checkout <a href="#additional-information">here</a> for more information.</p>

<p>To try it out, insert a prompt and click generate. The first run can be slow as the app needs to download and cache the weights (the second refresh will be faster as weights are cached by the browser).</p>

<script src="dist/tvmjs_runtime.wasi.js"></script>

<script src="dist/tvmjs.bundle.js"></script>

<script>
  var tvmjsGlobalEnv = tvmjsGlobalEnv || {};
</script>

<script type="module">
  import init, { TokenizerWasm } from "./dist/tokenizers-wasm/tokenizers_wasm.js";

  var initialized = false;
  async function getTokenizer(name) {
    if (!initialized) {
      await init();
    }
    const jsonText = await (await fetch("https://huggingface.co/" + name + "/raw/main/tokenizer.json")).text();
    return new TokenizerWasm(jsonText);
  }

  tvmjsGlobalEnv.getTokenizer = getTokenizer;
</script>

<script src="dist/stable_diffusion.js"></script>

<div>
  Input prompt: <input name="inputPrompt" id="inputPrompt" type="text" value="A photo of an astronaut riding a horse on mars" size="77" /> <br />
  Negative prompt (optional): <input name="negativePrompt" id="negativePrompt" type="text" value="" size="77" />
</div>

<div>
  Render intermediate steps (may slow down execution) -
  <select name="vae-cycle" id="vaeCycle">
    <option value="-1">No</option>
    <option value="2">Run VAE every two UNet steps after step 10</option>
  </select>

  <div id="progress">
    <label id="gpu-tracker-label"></label><br />
    <label id="progress-tracker-label"></label><br />
    <progress id="progress-tracker-progress" max="100" value="100"> </progress>
  </div>
  <button onclick="tvmjsGlobalEnv.asyncOnGenerate()">Generate</button>
</div>

<div>
<canvas id="canvas" width="512" height="512"></canvas>
</div>
<div id="log"></div>

<h2 id="additional-information">Additional Information</h2>

<p>You likely need to install the latest version of <a href="https://www.google.com/chrome/canary/">Chrome Canary</a>. To get the maximum performance, WebGPU works by translating WGSL shaders to native shaders. So in theory we can reach zero gap between the WebGPU runtime and native environment. If we directly use Chrome to check the current demo on Apple silicon however, we can find a performance degradation (about 3x). This is because Chrome’s WebGPU implementation inserts bound clips for all array index access, such that <code class="language-plaintext highlighter-rouge">a[i]</code> becomes <code class="language-plaintext highlighter-rouge">a[min(i, a.size)]</code>. Ideally downstream shader compilers should be able to optimize the bound clipping out, but here unfortunately it was not the case. This gap can be fixed once WebGPU implementation becomes more mature, checks the index access range, and drops such clipping.</p>

<p>You can get around this by using a special flag to launch chrome, by exiting chrome completely, then in command line, type</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/chrome --enable-dawn-features=disable_robustness
</code></pre></div></div>
<p>Then you will find that the execution speed is as fast as native gpu environment. These limitations are likely going to go away as WebGPU matures.</p>

<p>Please check out our <a href="https://github.com/mlc-ai/web-stable-diffusion">GitHub repo</a> for running the same shader flow locally on your GPU device through the native driver. Right now, there are still gaps, but we believe it is feasible to close such gaps as WebGPU dispatches to these native drivers.</p>

<h2 id="disclaimer">Disclaimer</h2>

<p>This demo site is for research purpose only. Please make sure to conform to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5#uses">uses of stable diffusion models</a>.</p>

            
        </div> <!-- /container -->

        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/web-stable-diffusion/assets/js/srcset-polyfill.js"></script>
    </body>

</html>
